import mediapipe as mp
from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np
import matplotlib.pyplot as plt
import cv2
import math
from typing import List, Dict, Tuple

def draw_landmarks_on_image(rgb_image, detection_result):
  face_landmarks_list = detection_result.face_landmarks
  annotated_image = np.copy(rgb_image)

  # Loop through the detected faces to visualize.
  for idx in range(len(face_landmarks_list)):
    face_landmarks = face_landmarks_list[idx]

    # Draw the face landmarks.
    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
    face_landmarks_proto.landmark.extend([
      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks
    ])

    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,
        landmark_drawing_spec=None,
        connection_drawing_spec=mp.solutions.drawing_styles
        .get_default_face_mesh_tesselation_style())
    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,
        landmark_drawing_spec=None,
        connection_drawing_spec=mp.solutions.drawing_styles
        .get_default_face_mesh_contours_style())
    solutions.drawing_utils.draw_landmarks(
        image=annotated_image,
        landmark_list=face_landmarks_proto,
        connections=mp.solutions.face_mesh.FACEMESH_IRISES,
          landmark_drawing_spec=None,
          connection_drawing_spec=mp.solutions.drawing_styles
          .get_default_face_mesh_iris_connections_style())

  return annotated_image

def calculate_distance(point1, point2):
    """ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2)

def calculate_angle(p1, p2, p3):
    """ì„¸ ì ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    a = calculate_distance(p2, p3)
    b = calculate_distance(p1, p3)
    c = calculate_distance(p1, p2)
    
    if a == 0 or b == 0 or c == 0:
        return 0
    
    cos_angle = (a**2 + b**2 - c**2) / (2 * a * b)
    cos_angle = max(-1, min(1, cos_angle))  # ë²”ìœ„ ì œí•œ
    return math.degrees(math.acos(cos_angle))

def analyze_emotion_from_landmarks(face_landmarks):
    """ì–¼êµ´ ëœë“œë§ˆí¬ì—ì„œ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    if not face_landmarks or len(face_landmarks) == 0:
        return {"emotion": "unknown", "confidence": 0.0, "details": {}}
    
    landmarks = face_landmarks[0]  # ì²« ë²ˆì§¸ ì–¼êµ´ ì‚¬ìš©
    
    # ì£¼ìš” ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (MediaPipe Face Mesh ê¸°ì¤€)
    # ëˆˆì¹, ëˆˆ, ì½”, ì… ê´€ë ¨ ëœë“œë§ˆí¬
    LEFT_EYE_INNER = 133
    LEFT_EYE_OUTER = 33
    RIGHT_EYE_INNER = 362
    RIGHT_EYE_OUTER = 263
    NOSE_TIP = 1
    MOUTH_LEFT = 61
    MOUTH_RIGHT = 291
    MOUTH_TOP = 13
    MOUTH_BOTTOM = 14
    LEFT_EYEBROW_INNER = 70
    LEFT_EYEBROW_OUTER = 46
    RIGHT_EYEBROW_INNER = 300
    RIGHT_EYEBROW_OUTER = 276
    
    try:
        # ëˆˆ í¬ê¸° ë¶„ì„ (ëˆˆì„ ëœ¨ê³  ìˆëŠ”ì§€)
        left_eye_width = calculate_distance(landmarks[LEFT_EYE_INNER], landmarks[LEFT_EYE_OUTER])
        right_eye_width = calculate_distance(landmarks[RIGHT_EYE_INNER], landmarks[RIGHT_EYE_OUTER])
        avg_eye_width = (left_eye_width + right_eye_width) / 2
        
        # ì… í¬ê¸° ë¶„ì„
        mouth_width = calculate_distance(landmarks[MOUTH_LEFT], landmarks[MOUTH_RIGHT])
        mouth_height = calculate_distance(landmarks[MOUTH_TOP], landmarks[MOUTH_BOTTOM])
        
        # ëˆˆì¹ ë†’ì´ ë¶„ì„
        left_eyebrow_height = landmarks[LEFT_EYEBROW_INNER].y - landmarks[LEFT_EYE_INNER].y
        right_eyebrow_height = landmarks[RIGHT_EYEBROW_INNER].y - landmarks[RIGHT_EYE_INNER].y
        avg_eyebrow_height = (left_eyebrow_height + right_eyebrow_height) / 2
        
        # ê°ì • ë¶„ì„ ë¡œì§
        emotion_scores = {
            "happy": 0.0,
            "sad": 0.0,
            "angry": 0.0,
            "surprised": 0.0,
            "neutral": 0.0,
            "confident": 0.0
        }
        
        # ë¯¸ì†Œ ê°ì§€ (ì…ê¼¬ë¦¬ê°€ ì˜¬ë¼ê°)
        mouth_corners_up = (landmarks[MOUTH_LEFT].y < landmarks[MOUTH_TOP].y and 
                           landmarks[MOUTH_RIGHT].y < landmarks[MOUTH_TOP].y)
        if mouth_corners_up and mouth_width > 0.02:  # ì…ì´ ì—´ë ¤ìˆê³  ë¯¸ì†Œ
            emotion_scores["happy"] += 0.4
            emotion_scores["confident"] += 0.2
        
        # ìŠ¬í”” ê°ì§€ (ëˆˆì¹ì´ ë‚´ë ¤ê°€ê³  ì…ê¼¬ë¦¬ê°€ ë‚´ë ¤ê°)
        if avg_eyebrow_height < -0.01:  # ëˆˆì¹ì´ ë‚´ë ¤ê°
            emotion_scores["sad"] += 0.3
        if not mouth_corners_up and mouth_width < 0.015:  # ì…ì´ ì‘ê³  ë‚´ë ¤ê°
            emotion_scores["sad"] += 0.2
        
        # í™”ë‚¨ ê°ì§€ (ëˆˆì¹ì´ ë‚´ë ¤ê°€ê³  ëˆˆì´ ì¢ì•„ì§)
        if avg_eyebrow_height < -0.005 and avg_eye_width < 0.025:  # ëˆˆì¹ ë‚´ë ¤ê°€ê³  ëˆˆ ì¢ìŒ
            emotion_scores["angry"] += 0.4
        
        # ë†€ëŒ ê°ì§€ (ëˆˆì´ í¬ê²Œ ëœ¨ì„)
        if avg_eye_width > 0.035:  # ëˆˆì´ í¬ê²Œ ëœ¨ì„
            emotion_scores["surprised"] += 0.4
        
        # ìì‹ ê° ê°ì§€ (ëˆˆì´ í¬ê²Œ ëœ¨ì´ê³  ë¯¸ì†Œ)
        if avg_eye_width > 0.03 and mouth_corners_up:
            emotion_scores["confident"] += 0.3
        
        # ì¤‘ë¦½ ê°ì§€ (ê¸°ë³¸ ìƒíƒœ)
        if (0.02 < avg_eye_width < 0.03 and 
            0.01 < mouth_width < 0.02 and 
            -0.005 < avg_eyebrow_height < 0.005):
            emotion_scores["neutral"] += 0.3
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ê°ì • ì„ íƒ
        max_emotion = max(emotion_scores, key=emotion_scores.get)
        max_score = emotion_scores[max_emotion]
        
        # ì‹ ë¢°ë„ ê³„ì‚° (0-1 ë²”ìœ„)
        confidence = min(1.0, max_score)
        
        return {
            "emotion": max_emotion,
            "confidence": confidence,
            "details": {
                "eye_width": avg_eye_width,
                "mouth_width": mouth_width,
                "mouth_height": mouth_height,
                "eyebrow_height": avg_eyebrow_height,
                "all_scores": emotion_scores
            }
        }
        
    except Exception as e:
        print(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"emotion": "unknown", "confidence": 0.0, "details": {}}

def get_interview_feedback(emotion_data):
    """ë©´ì ‘ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤."""
    emotion = emotion_data["emotion"]
    confidence = emotion_data["confidence"]
    
    feedback = {
        "overall_score": 0,
        "feedback": "",
        "suggestions": []
    }
    
    if emotion == "confident":
        feedback["overall_score"] = 90
        feedback["feedback"] = "ìì‹ ê° ìˆëŠ” í‘œì •ì…ë‹ˆë‹¤! ì¢‹ì€ ë©´ì ‘ íƒœë„ì…ë‹ˆë‹¤."
        feedback["suggestions"] = ["ê³„ì† ì´ í‘œì •ì„ ìœ ì§€í•˜ì„¸ìš”."]
        
    elif emotion == "happy":
        feedback["overall_score"] = 85
        feedback["feedback"] = "ê¸ì •ì ì´ê³  ë°ì€ í‘œì •ì…ë‹ˆë‹¤."
        feedback["suggestions"] = ["ìì—°ìŠ¤ëŸ¬ìš´ ë¯¸ì†Œë¥¼ ìœ ì§€í•˜ì„¸ìš”."]
        
    elif emotion == "neutral":
        feedback["overall_score"] = 70
        feedback["feedback"] = "ì°¨ë¶„í•œ í‘œì •ì…ë‹ˆë‹¤."
        feedback["suggestions"] = ["ì¡°ê¸ˆ ë” ë°ì€ í‘œì •ì„ ì—°ìŠµí•´ë³´ì„¸ìš”.", "ìì—°ìŠ¤ëŸ¬ìš´ ë¯¸ì†Œë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”."]
        
    elif emotion == "surprised":
        feedback["overall_score"] = 60
        feedback["feedback"] = "ë†€ë€ í‘œì •ì´ ë³´ì…ë‹ˆë‹¤."
        feedback["suggestions"] = ["ë” ì°¨ë¶„í•˜ê³  ì•ˆì •ì ì¸ í‘œì •ì„ ì—°ìŠµí•´ë³´ì„¸ìš”."]
        
    elif emotion == "sad":
        feedback["overall_score"] = 40
        feedback["feedback"] = "ìš°ìš¸í•˜ê±°ë‚˜ í”¼ê³¤í•´ ë³´ì…ë‹ˆë‹¤."
        feedback["suggestions"] = ["ì¶©ë¶„í•œ íœ´ì‹ì„ ì·¨í•˜ì„¸ìš”.", "ê¸ì •ì ì¸ ë§ˆìŒê°€ì§ì„ ê°€ì ¸ë³´ì„¸ìš”."]
        
    elif emotion == "angry":
        feedback["overall_score"] = 30
        feedback["feedback"] = "í™”ê°€ ë‚˜ê±°ë‚˜ ê¸´ì¥ëœ í‘œì •ì…ë‹ˆë‹¤."
        feedback["suggestions"] = ["ì‹¬í˜¸í¡ì„ í•˜ê³  ê¸´ì¥ì„ í’€ì–´ë³´ì„¸ìš”.", "ê¸ì •ì ì¸ ìƒê°ì„ í•´ë³´ì„¸ìš”."]
        
    else:
        feedback["overall_score"] = 50
        feedback["feedback"] = "í‘œì •ì„ ëª…í™•íˆ ë¶„ì„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
        feedback["suggestions"] = ["ë” ëª…í™•í•œ í‘œì •ì„ ì—°ìŠµí•´ë³´ì„¸ìš”."]
    
    # ì‹ ë¢°ë„ì— ë”°ë¥¸ ì¡°ì •
    if confidence < 0.5:
        feedback["suggestions"].append("ë” ëª…í™•í•œ í‘œì •ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”.")
    
    return feedback

def create_emotion_visualization(emotion_data, feedback):
    """ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    emotion = emotion_data['emotion']
    confidence = emotion_data['confidence']
    score = feedback['overall_score']
    
    # ê°ì •ë³„ ìƒ‰ìƒ ë§¤í•‘
    emotion_colors = {
        'happy': '#FFD700',      # ê¸ˆìƒ‰
        'confident': '#32CD32',  # ë¼ì„ê·¸ë¦°
        'neutral': '#87CEEB',    # í•˜ëŠ˜ìƒ‰
        'surprised': '#FFA500',  # ì˜¤ë Œì§€
        'sad': '#4169E1',        # ë¡œì–„ë¸”ë£¨
        'angry': '#DC143C',      # í¬ë¦¼ìŠ¨
        'unknown': '#808080'     # íšŒìƒ‰
    }
    
    # ê°ì • ì ìˆ˜ ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±
    emotions = ['happy', 'confident', 'neutral', 'surprised', 'sad', 'angry']
    scores = [emotion_data['details']['all_scores'].get(emotion, 0) for emotion in emotions]
    colors = [emotion_colors[emotion] for emotion in emotions]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ê°ì • ì ìˆ˜ ë§‰ëŒ€ ê·¸ë˜í”„
    bars = ax1.bar(emotions, scores, color=colors, alpha=0.7)
    ax1.set_title('ê°ì • ë¶„ì„ ì ìˆ˜', fontsize=14, fontweight='bold')
    ax1.set_ylabel('ì ìˆ˜')
    ax1.set_ylim(0, 1)
    
    # ë§‰ëŒ€ ìœ„ì— ì ìˆ˜ í‘œì‹œ
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{score:.2f}', ha='center', va='bottom')
    
    # ì „ì²´ ì ìˆ˜ ì›í˜• ê·¸ë˜í”„
    ax2.pie([score, 100-score], labels=['ì ìˆ˜', 'ë‚¨ì€ ì ìˆ˜'], 
            colors=[emotion_colors.get(emotion, '#808080'), '#E0E0E0'],
            autopct='%1.0f%%', startangle=90)
    ax2.set_title(f'ë©´ì ‘ í‘œì • ì ìˆ˜: {score}/100', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('emotion_analysis_result.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š ê°ì • ë¶„ì„ ì‹œê°í™”ê°€ 'emotion_analysis_result.png'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def plot_face_blendshapes_bar_graph(face_blendshapes):
  # Extract the face blendshapes category names and scores.
  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]
  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]
  # The blendshapes are ordered in decreasing score value.
  face_blendshapes_ranks = range(len(face_blendshapes_names))

  fig, ax = plt.subplots(figsize=(12, 12))
  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])
  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)
  ax.invert_yaxis()

  # Label each bar with values
  for score, patch in zip(face_blendshapes_scores, bar.patches):
    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f"{score:.4f}", va="top")

  ax.set_xlabel('Score')
  ax.set_title("Face Blendshapes")
  plt.tight_layout()
  plt.show()


def main():
    # STEP 1: Import the necessary modules.
    from mediapipe.tasks import python
    from mediapipe.tasks.python import vision

    # STEP 2: Create an FaceLandmarker object.
    base_options = python.BaseOptions(model_asset_path='./models/face_landmarker.task')
    options = vision.FaceLandmarkerOptions(base_options=base_options,
                                           output_face_blendshapes=True,
                                           output_facial_transformation_matrixes=True,
                                           num_faces=1)
    detector = vision.FaceLandmarker.create_from_options(options)

    # STEP 3: Load the input image.
    image = mp.Image.create_from_file("face.jpeg")

    # STEP 4: Detect face landmarks from the input image.
    detection_result = detector.detect(image)

    # STEP 5: Process the detection result. In this case, visualize it.
    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)
    
    # STEP 6: Analyze emotion from face landmarks
    print("\n" + "="*50)
    print("ğŸ­ ë©´ì ‘ í‘œì • ë¶„ì„ ê²°ê³¼")
    print("="*50)
    
    emotion_data = analyze_emotion_from_landmarks(detection_result.face_landmarks)
    
    print(f"ğŸ“Š ê°ì • ë¶„ì„:")
    print(f"   ê°ì •: {emotion_data['emotion']}")
    print(f"   ì‹ ë¢°ë„: {emotion_data['confidence']:.2f}")
    
    if emotion_data['details']:
        details = emotion_data['details']
        print(f"   ëˆˆ í¬ê¸°: {details['eye_width']:.4f}")
        print(f"   ì… í¬ê¸°: {details['mouth_width']:.4f}")
        print(f"   ëˆˆì¹ ë†’ì´: {details['eyebrow_height']:.4f}")
    
    # STEP 7: Get interview feedback
    feedback = get_interview_feedback(emotion_data)
    
    print(f"\nğŸ“ ë©´ì ‘ í”¼ë“œë°±:")
    print(f"   ì ìˆ˜: {feedback['overall_score']}/100")
    print(f"   í‰ê°€: {feedback['feedback']}")
    print(f"   ì¡°ì–¸:")
    for i, suggestion in enumerate(feedback['suggestions'], 1):
        print(f"   {i}. {suggestion}")
    
    # STEP 8: Create emotion visualization
    try:
        create_emotion_visualization(emotion_data, feedback)
    except Exception as e:
        print(f"ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    # STEP 9: Save the result image
    output_path = "face_landmarks_result.jpg"
    cv2.imwrite(output_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))
    print(f"\nğŸ’¾ ì–¼êµ´ ëœë“œë§ˆí¬ ê²°ê³¼ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # Optional: Try to display if GUI is available
    try:
        cv2.imshow("Face Landmarks", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))
        cv2.waitKey(0)
        cv2.destroyAllWindows()
    except Exception as e:
        print(f"GUI í‘œì‹œ ë¶ˆê°€ (WSL2 í™˜ê²½): {e}")
        print("ê²°ê³¼ ì´ë¯¸ì§€ëŠ” íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("="*50)

if __name__ == "__main__":
    main()